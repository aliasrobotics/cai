{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9edb6753",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import json\n",
    "from collections import defaultdict\n",
    "import os\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cebb746",
   "metadata": {},
   "outputs": [],
   "source": [
    "#UTIL Funcitons to calculate metrics\n",
    "\n",
    "def read_results(main_folder=\"results\",model=None):\n",
    "    \"\"\"\n",
    "    Reads the results from the specified main folder and organizes them into a nested dictionary structure.\n",
    "    \n",
    "    Args:\n",
    "        main_folder (str): The path to the main folder containing the results.\n",
    "        \n",
    "    Returns:\n",
    "        list: list with dictionaries with agents, sections, and their corresponding logs.\n",
    "    \"\"\"\n",
    "    results = defaultdict(lambda: defaultdict(dict))\n",
    "    list_data = []\n",
    "    agents = os.listdir(os.path.join(main_folder,model))\n",
    "    for agent in agents:\n",
    "            sections = os.listdir(os.path.join(main_folder,model,agent))\n",
    "            for section in sections:\n",
    "                statuses = os.listdir(os.path.join(main_folder,model,agent,section))\n",
    "                for status in statuses:\n",
    "                    labs = os.listdir(os.path.join(main_folder,model,agent,section,status))\n",
    "                    for lab in labs:\n",
    "                        try:\n",
    "                            file = os.listdir(os.path.join(main_folder,model,agent,section,status,lab))[0]\n",
    "                        except IndexError:\n",
    "                             print(os.listdir(os.path.join(main_folder,model,agent,section,status,lab)))\n",
    "                        with open(os.path.join(main_folder,model,agent,section,status,lab,file)) as f:\n",
    "                                    logs = [json.loads(line) for line in f]\n",
    "                                    \n",
    "                        data = {\n",
    "                                'agent':agent,\n",
    "                                'section':section,\n",
    "                                'model':model,\n",
    "                                'lab title':lab,\n",
    "                                'status':status,\n",
    "                                'logs':logs\n",
    "                                }                                  \n",
    "                        list_data.append(data)\n",
    "    return list_data\n",
    "\n",
    "\n",
    "def get_metrics(labs):\n",
    "    \"\"\"\n",
    "    Extracts metrics from the provided list of lab results.\n",
    "    \n",
    "    Args:\n",
    "        labs (list): A list of dictionaries containing lab results, where each dictionary includes logs and metadata.\n",
    "        \n",
    "    Returns:\n",
    "        list: A list of dictionaries containing calculated metrics for each lab\n",
    "    \"\"\"\n",
    "    \n",
    "    results = []\n",
    "    for lab in labs:\n",
    "        \n",
    "        #------- DATA EXTRACTION --------\n",
    "        completitions = [log for log in lab['logs'] if log.get('object') == 'chat.completion']\n",
    "        user_messages = [log for log in lab['logs'] if log.get(\"event\") == \"user_message\" ]\n",
    "        assistant_messages = [log for log in lab['logs'] if log.get(\"event\") == \"assistant_message\" ]\n",
    "        model_metadata = [log for log in lab['logs'] if \"model\" in log ]\n",
    "\n",
    "        #model\n",
    "        model = model_metadata[0]['model']\n",
    "\n",
    "        #assistant messages \n",
    "        assistant_contents = [\n",
    "            choice['message']['content']\n",
    "            for co in completitions\n",
    "            for choice in co['choices']\n",
    "        ]\n",
    "\n",
    "        #assistant tools\n",
    "        assistant_tools_calls = [\n",
    "            tool['function']\n",
    "            for co in completitions\n",
    "            for choice in co['choices']\n",
    "            for tool in choice['message']['tool_calls']\n",
    "        ]\n",
    "\n",
    "        #finish reason\n",
    "        finish_reasons = [\n",
    "            choice['finish_reason']\n",
    "            for co in completitions\n",
    "            for choice in co['choices']\n",
    "        ]\n",
    "\n",
    "        #integration of finish reason, assistant_contents, and assistant_tools_calls\n",
    "        assistant_outputs = [{\"message\":a, \"finish_reason\":b,\"tool\":c} for a, b, c in zip(assistant_contents, finish_reasons,assistant_tools_calls)]\n",
    "\n",
    "\n",
    "        #------- METRICS CALCULATION --------\n",
    "        #turns\n",
    "        total_turns = len(user_messages)\n",
    "\n",
    "        #time\n",
    "        active_seconds = [ac['timing']['active_seconds'] for ac in completitions]\n",
    "        idle_seconds = [ac['timing']['idle_seconds'] for ac in completitions]\n",
    "        total_active_seconds = sum(active_seconds)\n",
    "        total_idle_seconds = sum(idle_seconds) \n",
    "        total_seconds = total_active_seconds + total_idle_seconds\n",
    "\n",
    "        #tokens\n",
    "        prompt_tokens = [ac['usage']['prompt_tokens'] for ac in completitions]\n",
    "        completion_tokens = [ac['usage']['completion_tokens'] for ac in completitions]\n",
    "        total_prompt_tokens = sum(prompt_tokens)\n",
    "        total_completion_tokens = sum(completion_tokens)\n",
    "        total_tokens = total_prompt_tokens + total_completion_tokens\n",
    "\n",
    "        #costs\n",
    "        interaction_costs = [ac['cost']['interaction_cost'] for ac in completitions]\n",
    "        total_interaction_costs = sum(interaction_costs)\n",
    "\n",
    "        #assistant outputs\n",
    "        total_assistant_messages = len([x for x in assistant_contents if x is not None])\n",
    "\n",
    "        #assistant tools\n",
    "        total_assistant_tools = len([x for x in assistant_tools_calls])\n",
    "\n",
    "        metrics = {\n",
    "            \"agent\": lab['agent'],\n",
    "            \"section\": lab['section'],\n",
    "            \"model\": lab['model'],\n",
    "            \"lab_title\": lab['lab title'],\n",
    "            \"status\": lab['status'],\n",
    "            \"turns\": total_turns,\n",
    "            \"active_seconds\": total_active_seconds,\n",
    "            \"idle_seconds\": total_idle_seconds,\n",
    "            \"total_seconds\": total_seconds,\n",
    "            \"prompt_tokens\": total_prompt_tokens,\n",
    "            \"completion_tokens\": total_completion_tokens,\n",
    "            \"total_tokens\": total_tokens,\n",
    "            \"interaction_costs\": total_interaction_costs,\n",
    "            \"total_assistant_messages\": total_assistant_messages,\n",
    "            \"total_assistant_tools\": total_assistant_tools,\n",
    "            \"assistant_outputs\": json.dumps(assistant_outputs) \n",
    "        }\n",
    "        results.append(metrics)\n",
    "    return results "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "766a3f90",
   "metadata": {},
   "source": [
    "# Define the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73c2a8b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv('.env')\n",
    "\n",
    "MODEL = os.getenv(\"CAI_MODEL\").replace('/','-')\n",
    "MODEL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d69e2850",
   "metadata": {},
   "source": [
    "<h1>1. Read results and generate metrics tables</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "131d62a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = read_results(model=MODEL)\n",
    "df_metrics = pd.DataFrame(get_metrics(results))\n",
    "\n",
    "#calcualte the mean of the metrics\n",
    "mean_metrics = (df_metrics.drop(columns=['status',\n",
    "                                         'lab_title',\n",
    "                                         'assistant_outputs'\n",
    "                            ]).groupby(['agent', \n",
    "                                        'section', \n",
    "                                        'model'])\n",
    "                            .mean()\n",
    "                            .reset_index())\n",
    "\n",
    "\n",
    "#calculate the sum of status metric\n",
    "df_metrics = pd.get_dummies(df_metrics, columns=['status'],prefix='',prefix_sep='')\n",
    "if 'interrupted' not in df_metrics.columns:\n",
    "    df_metrics['interrupted'] = False\n",
    "if 'not-solved' not in df_metrics.columns:\n",
    "    df_metrics['not-solved'] = False\n",
    "if 'solved' not in df_metrics.columns:\n",
    "    df_metrics['solved'] = False\n",
    "\n",
    "df_metrics[['interrupted','not-solved','solved']] = df_metrics[['interrupted','not-solved','solved']].astype(int)\n",
    "status_metrics = (df_metrics.drop(columns=['lab_title',\n",
    "                                          'assistant_outputs'])\n",
    "                            .groupby(['agent', \n",
    "                                      'section', \n",
    "                                      'model'])\n",
    "                            [['interrupted','not-solved','solved']]\n",
    "                            .sum()\n",
    "                            .reset_index())\n",
    "\n",
    "\n",
    "\n",
    "df_calculated_metrics = pd.merge(mean_metrics, status_metrics, on=['agent', 'section', 'model'])\n",
    "df_calculated_metrics = df_calculated_metrics.rename(columns={\n",
    "    'turns': 'avg_turns',\n",
    "    'active_seconds': 'avg_active_seconds',\n",
    "    'idle_seconds': 'avg_idle_seconds',\n",
    "    'total_seconds': 'avg_total_seconds',\n",
    "    'prompt_tokens': 'avg_prompt_tokens',\n",
    "    'completion_tokens': 'avg_completion_tokens',\n",
    "    'total_tokens': 'avg_total_tokens',\n",
    "    'interaction_costs': 'avg_interaction_costs', \n",
    "    'total_assistant_messages': 'avg_total_assistant_messages',\n",
    "    'total_assistant_tools': 'avg_total_assistant_tools',   \n",
    "    'interrupted': 'total_interrupted',\n",
    "    'not-solved': 'total_not_solved',\n",
    "    'solved': 'total_solved'\n",
    "})\n",
    "\n",
    "#save the dataframe to a excel file\n",
    "df_metrics.to_excel(f'metrics_experiment/evaluation_metrics_{MODEL}.xlsx', index=False)\n",
    "df_calculated_metrics.to_excel(f'metrics_experiment/calculated_evaluation_metrics_{MODEL}.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9a9c75a",
   "metadata": {},
   "source": [
    "<h1>2. Graph Assistant Messages and Tools by Agent</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da6b875c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_calculated_metrics[['agent','avg_total_assistant_messages','avg_total_assistant_tools']].groupby('agent').mean().round(1).reset_index()\n",
    "\n",
    "# Plotting\n",
    "x = range(len(df))\n",
    "width = 0.35\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "bars1 = ax.bar([i - width/2 for i in x], df['avg_total_assistant_messages'], width,\n",
    "               label='Avg Assistant Messages', color='gray')\n",
    "bars2 = ax.bar([i + width/2 for i in x], df['avg_total_assistant_tools'], width,\n",
    "               label='Avg Assistant Tools', color='white', edgecolor='black')\n",
    "\n",
    "# Labels and legend\n",
    "ax.set_xlabel('Agent Type')\n",
    "ax.set_ylabel('Average Count')\n",
    "ax.set_title('Assistant Messages and Tools by Agent Type')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(df['agent'])\n",
    "ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73457506",
   "metadata": {},
   "source": [
    "<h1>2. Graph Lab Status by Agent Type and Lab Type</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dd1cba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_calculated_metrics[['agent','section','total_interrupted','total_not_solved','total_solved']].groupby(['agent','section']).sum().reset_index()\n",
    "df['section'] = df['section'].map({'cross-site-request-forgery-csrf':'CSRF','cross-site-scripting':'XSS','sql-injection':'SQLI'})\n",
    "\n",
    "# Setup\n",
    "prompts = df['agent'].unique()\n",
    "sections = df['section'].unique()\n",
    "\n",
    "width = 0.25\n",
    "x = range(len(sections))\n",
    "\n",
    "for prompt in prompts:\n",
    "    df_prompt = df[df['agent'] == prompt]\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(7, 4))\n",
    "\n",
    "    ax.bar([i - width for i in x], df_prompt['total_interrupted'], width, label='Interrupted', color='gray')\n",
    "    ax.bar(x, df_prompt['total_not_solved'], width, label='Not Solved', color='white', edgecolor='black')\n",
    "    ax.bar([i + width for i in x], df_prompt['total_solved'], width, label='Solved', color='lightgray')\n",
    "\n",
    "    ax.set_title(prompt)\n",
    "    ax.set_ylabel('Total Count')\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(df_prompt['section'], rotation=30, ha='right')\n",
    "\n",
    "    # Move legend outside\n",
    "    ax.legend(loc='center left', bbox_to_anchor=(1, 0.5), frameon=True)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60127c56",
   "metadata": {},
   "source": [
    "<h1>3. Seconds by Agent Type</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8037192e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_calculated_metrics[['agent','avg_active_seconds','avg_idle_seconds']].groupby('agent').mean().round(1).reset_index()\n",
    "\n",
    "# Plotting\n",
    "x = range(len(df))\n",
    "width = 0.35\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "bars1 = ax.bar([i - width/2 for i in x], df['avg_active_seconds'], width,\n",
    "               label='Avg Active Seconds', color='gray')\n",
    "bars2 = ax.bar([i + width/2 for i in x], df['avg_idle_seconds'], width,\n",
    "               label='Avg Idle Seconds', color='white', edgecolor='black')\n",
    "\n",
    "# Labels and legend\n",
    "ax.set_xlabel('Agent Type')\n",
    "ax.set_ylabel('Average Count')\n",
    "ax.set_title('Active and Idle Seconds by Agent Type')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(df['agent'])\n",
    "ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "318633f0",
   "metadata": {},
   "source": [
    "<h1>4.Tokens by Agent Type</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8886fa21",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_calculated_metrics[['agent','avg_prompt_tokens','avg_completion_tokens']].groupby('agent').mean().round(1).reset_index()\n",
    "\n",
    "# Plotting\n",
    "x = range(len(df))\n",
    "width = 0.35\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "bars1 = ax.bar([i - width/2 for i in x], df['avg_prompt_tokens'], width,\n",
    "               label='Avg Prompt Tokens', color='gray')\n",
    "bars2 = ax.bar([i + width/2 for i in x], df['avg_completion_tokens'], width,\n",
    "               label='Avg Idle Seconds', color='white', edgecolor='black')\n",
    "\n",
    "# Labels and legend\n",
    "ax.set_xlabel('Agent Type')\n",
    "ax.set_ylabel('Average Count')\n",
    "ax.set_title('Prompt and Completion Tokens by Agent Type')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(df['agent'])\n",
    "ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b685e2b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_calculated_metrics[['agent','section','avg_turns']].groupby(['agent','section']).mean().reset_index()\n",
    "df['section'] = df['section'].map({'cross-site-request-forgery-csrf':'CSRF','cross-site-scripting':'XSS','sql-injection':'SQLI'})\n",
    "\n",
    "# Unique prompts\n",
    "agents = df['agent'].unique()\n",
    "\n",
    "\n",
    "for agent in agents:\n",
    "    df_agent = df[df['agent'] == agent]\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(6, 4))\n",
    "\n",
    "    bars = ax.bar(df_agent['section'], df_agent['avg_turns'],\n",
    "                  color='gray', edgecolor='black', label='Avg Turns')\n",
    "\n",
    "    # Add value labels\n",
    "    for bar in bars:\n",
    "        yval = bar.get_height()\n",
    "        ax.text(bar.get_x() + bar.get_width()/2, yval + 0.1, f'{yval:.1f}', ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "    ax.set_title(agent)\n",
    "    ax.set_ylabel('Average Turns')\n",
    "    ax.set_xlabel('Section')\n",
    "    ax.set_ylim(0, max(df['avg_turns']) + 1)\n",
    "    ax.set_xticklabels(df_agent['section'], rotation=30, ha='right')\n",
    "\n",
    "    # Show legend\n",
    "    ax.legend(loc='upper left')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46721930",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "uni",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
